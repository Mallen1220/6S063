<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>6.S063 Engineering Interactive Technologies</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index.html">Trancendance</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            Menu
            <i class="fa fa-bars"></i>
        </button>
          <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
              <li class="nav-item">
                <a class="nav-link" href="ideation.html">Leading up to Trancendance</a>
              </li>
            </ul>
          </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('img/ContourInvert.jpg')">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>6.S063 Engineering Interactive Technologies</h1>
              <h2 class="subheading">Max Allen and Tim Chong</h2>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
              <!--
            <h2 class="section-heading">Table Of Contents:</h2>
            <ol>
                <li>
                    <a href="#brainstorm">Final Project Brainstorming</a>
                </li>
                <li>
                    <a href="#choosing">Choosing a Final Project</a>
                </li>
                <li>
                    <a href="#fp">Final Project: Trancendance</a>
                </li>
            </ol>
            <h2 class="section-heading">Quicklinks:</h2>
            <ul>
                <li>
                    <a href="https://docs.google.com/presentation/d/1ltuzCreSogl7y6m78S14Pl02JILJBi6kERdp65kSTCk/edit?usp=sharing">Ideation Presentation</a>
                </li>
                <li>
                    <a href="https://docs.google.com/presentation/d/1eqS8FEjmtxZL1Nq0hjdQx3wjD5r3OosZnqxppWZnAug/edit?usp=sharing">Midterm Presentation</a>
                </li>
                <li>
                    <a href="https://docs.google.com/presentation/d/1KsIUj3R7Sw_sGRQCA6rZZQzdCT8cnYpHLDQY7wdK7oM/edit?usp=sharing">Final Presentation</a>
                </li>
            </ul>
            <hr style="margin-top:50px">
        -->
            <h1>Trancendance</h1>
            <p>
                Trancendance is a brain-computer system that detects a users state of mind through EEG signal processing, and augments the users physical environment to achieve a desired state of mind. 
            </p>
            <iframe src="https://player.vimeo.com/video/247149313" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
            <h2 class="section-heading">The Problem</h2>
            <p>
                The problem Trancendance is tackling is the problem of <i><b>distraction</b></i>. We are constantly surrounded by distractions of all kinds: cacophonous sounds, bright lights and screens, weird smells, and temptations that draw our mind away from relaxation and focus. 
            </p>
            <p>
                <i><b>We believe</b></i> that by understanding what environments are conducive to our different states of mind, we will be able to influence our mental state by re-creating those environments.
            </p>
            <p>
                <i><b>We envision</b></i> a world where our environment changes on demand to fit our desired state of mind. 
            </p>
            <h2 class="section-heading">How Trancendance Works</h2>
            <img class="img-fluid" src="img/IMG_1525.JPG" alt="">
            <p>
                Trancendance is the first step in achieving our goal of controlling our state of mind. Transcendance is the combination of a EEG brain-sensing headset plus configuration software that allows the headset to calibrate how your brain reacts to being in different environments. We are <i>narrowly</i> defining the term environment to be the combination of light and music in your room because those are the variables we targeted to control for the first iteration of Trancendance. After a calibration period, Trancendance will store which environment was most conducive to focusing and which environment was most conducive to relaxing. When you want to relax, just tell Trancendance, and the system will adjust your environment to match the calibrated setting. Same thing with being focused.
            </p>
            <blockquote class="blockquote">Why focus and relaxation?</blockquote>
            <p>
                We chose to measure for focus and relaxation states within the brain because there are known relationships of brain wave frequencies that correspond with these states of mind. Distinguishing other states of mind including excitement, sadness, anger, etc. are more difficult and something we're interested in exploring further after this class.
            </p>
            <h2 class="section-heading">System Architecture</h2>
            <p>
                Our system is broken up into 3 main groups:
                <ul>
                    <li>
                        BCI Headset
                    </li>
                    <li>
                        Processing and Control Software
                    </li>
                    <li>
                        Audio and Visual Feedback
                    </li>
                </ul>
                These three main groups correspond to the flow of information in the below graphic:
            </p>
            <img class="img-fluid" src="img/SysArch.png" alt="">
            <h2 class="section-heading">Our Brain-Sensing Headset</h2>
            <img class="img-fluid" src="img/IMG_3960.JPG" alt="">
            <p>
                Our EEG Headset is made up of combining an OpenBCI bio-sensing board, the <a href="https://shop.openbci.com/collections/frontpage/products/pre-order-ganglion-board">Ganglion Board</a>, with <a href="https://shop.openbci.com/collections/frontpage/products/openbci-gold-cup-electrodes">Gold Cup Electrodes</a>, a 3D printed and clear acrylic enclosure, and a comfortable hat. We cut holes in the hat and tied the Ganglion Board enclosure to the hat so nothing would fall off. Additionally, we use the natural lip of the hat to hold the long EEG wires. The Ganglion Board is powered by a compact Li-ion 3.7V battery that hides in the back of the enclosure for pleasant and compact design
            </p>
            <h2 class="section-heading">Interfacing with the OpenBCI Ganglion</h2>
            <p>
                Our Brain-sensing headset interfaces with our control computer via Bluetooth Low Energy. The board, once it's turned on, can directly connect to the OpenBCI graphical user interface (GUI). In the OpenBCI GUI, there are fantastic visualization capabilities and settings to view everything that the Ganglion board is sensing. However, Trancendance only uses this GUI as a intermediate processing and networking black box. The GUI automatically performs the fast-fourier tranform on the raw brain signal to get the frequency spectrum of one's brainwaves. From there, the frequencies are combined into bins of frequencies that correspond to types of <a href="https://en.wikipedia.org/wiki/Neural_oscillation#Brain%E2%80%93computer_interface">neurological oscillatory activities</a> called alpha, beta, theta, delta, and gamma waves. The levels of these bands of waves are then networked via the <a href="http://opensoundcontrol.org/introduction-osc">OSC Protocol</a> to our Python script that receives OSC packets and parses the neurological oscillatory activity. 
            </p>
            <h2 class="section-heading">Processing the Brain data</h2>
            <p>
                Our Python script, besides acting as a command line interface to calibrate and control Trancendance, does the brain signal processing to determine which music and lighting environment makes the user most relaxed and most focused.
            </p>
            <p>
                <!-- TIM TODO: Write about brain processing algorithm and electrode placement and 10-20 system-->
            </p>
            <p>
                When the python script wants to modify or change the environment, it sends a simple HTTP request to our Node.js server that runs on our local machine. The Node.js server is used to easily interface with the Spotify and Hue APIs
            </p>
            <h2 class="section-heading">Interfacing with the Spotify API:</h2>
            <p>
                We easily interface with the Spotify API by using a Node.js npm package called <a href="https://github.com/thelinmichael/spotify-web-api-node/blob/master/src/spotify-web-api.js">Spotify Web API Node</a> by thelinmichael on Github. Before you can use that library, you have to register and set up an application with Spotify Developer. From there, you will get the necesary application ID and secret necessary to request information from Spotify. The <a href="https://developer.spotify.com/web-api/tutorial/">Spotify WebAPI Starter Tutorial</a> was massively helpful in getting us started. From there, it's a matter of understanding <a href="https://developer.spotify.com/web-api/using-scopes/">scopes</a> of different API endpoints, <a href="https://developer.spotify.com/web-api/authorization-guide/">user authorization and authentication</a>, and the <a href="https://developer.spotify.com/web-api/web-api-connect-endpoint-reference/">Spotify Web API</a>.
            </p>

            <h2 class="section-heading">Interfacing with the Hue API:</h2>
            <p>
                The hardest part about communicating with the Hue light system was actually setting the system up. The <a href="http://www2.meethue.com/en-us/p/hue-bridge/046677458478">Hue Bridge</a> is meant to plug into your home router to connect directly to the internet. This way, when you're connected to the same network on your phone, your phone's app (or your Amazon Alexa or Google Home) just has to send a message over the local network to the Hue Bridge. What that means, though is two important things:
                <ol>
                    <li>
                        You have to have a ethernet connection for your Hue Bridge to plug into.
                    </li>
                    <li>
                        You can only control your lights using a device connected to the same network that your Hue Bridge is on.
                    </li>
                </ol>
                The reason these two things makes things difficult is that at a college like MIT, the internet networking is extremely complicated. While you might only see three networks listed as "MIT Guest," "MIT," and "MIT Secure," there are actualls dozens of networks all named the same exact thing to extend the most amount of wireless internet access across the whole campus. What's worse is that the network you connect to in your ethernet port in the wall in your dormroom or some classroom and the wireless networks broadcasted might be completely different networks.
            </p>
            <p>
                Our solution to this was to get a small little travel router that we can plug into the ethernet ports in the wall, and then broadcast our own new network so that the Hue Bridge, our computer, and our phones could all be connected to the travel router's spoofed network. The issue we ran into here was that MIT restricts people from doing stuff like this. We ran into multiple issues trying to set up these networks, getting constant redirects to MIT's IS&amp;T Network Registration page. After trying to register with IS&amp;T, we still found restricted access to websites like Google and Facebook, while we could access sites like <a href="http://mitadmissions.org/">MIT Admissions</a> and <a href="https://www.wikipedia.org/">Wikipedia</a>. Hmmm...
            </p>
            <p>
                After moving from one of the computer clusters on campus to the IDC to test our network there, things were suddenly ok and we could seemingly access any webpage over our network, and the Hue Bridge was able to successfully connect. After setting up our Hue light in the iPhone App, we used <a href="https://www.developers.meethue.com/documentation/getting-started">Hue API Documentation</a> to connect to the Bridge API debug interface as shown below:
            </p>
            <img class="img-fluid" src="img/Screen Shot 2017-12-14 at 11.00.48 AM.png" alt="">
            <p>
                From here we got a sense of what types of commands we could send to the bridge, and what the JSON structures looked like for individual lights and groups of lights. To interface with the API more easily, we ended up using a Node.js <a href="https://github.com/peter-murray/node-hue-api#setting-light-states-using-custom-json-object">Node Hue API</a> package from peter-murray on Github. Similar to the Spotify API, it essentailly took in some configuration details like the Bridge IP address and Username and would allow us to easily send and receive JSON messages with the Bridge to control light states.
            </p>
            <h2 class="section-heading">Challenges:</h2>
            <p>
                The primary challenges when building and testing Trancendance were as follows:
                <ul>
                    <li>
                        Getting good EEG readings while minimizing prep time
                    </li>
                    <li>
                        Distinguishing mental states from EEG signals
                    </li>
                </ul>
                While we tried to make the EEG headset very sleek and comfortable (which we believe we succeeded in), the amount of preparation to put on the EEG electrodes was still way more than we'd like. In order to get a good signal from the electrodes, there needs to be very good conductivity between the cup electrode and the users skin. Just taping the electrodes to one's head wouldn't work well since the resistance and impedance between the metal and the actual firing neurons was too high. To lower this, we added electrode gel to the inside of the cups. This definitely helped us get a good signal, but unfortunately added an unwanted amount of mess and preparation. Our original vision of someone just putting on a hat and having the brain-sensing work was debunked.
            </p>
            <p>
                Additionally, while researchers generally have a good sense of what brainwaves look like for focused and relaxed states of mind, other states of mind don't have quite as prescreptive a "footprint" as the above two, and so incorperating these into the first iteration of Trancendance was infeasible.
            </p>
            <h2 class="section-heading">Future Outlook:</h2>
            <p>
                The great thing about having the above challenges is knowing that these are the types of fundamental problems that neurological research is helping to tackle. We are confident with several more years of research and development, brain interfaces and applications similar to Trancendance will emerge in a more wide-spread manner.
            </p>
            <p>
                Additionally, with the proliferation of augmented reality headsets and systems, perhaps our definition of an <i>"environment"</i> that one can control will be vastly expanded to include one's augmented environment. Imagine if Trancendance knew you were in a focused state and could cause your AR headset to block out view of your cell phone, unhealthy food, or any other type of distraction? With the power of that kind of system, what would you build?
            </p>
            <h2 class="section-heading">Special Thank You</h2>
            <p>
                A <b><i>massive</i></b> thank you goes out to Stefanie Mueller and Jared Countsfor putting on this class, 6.S063, at MIT for the first semester! We hope this class and the projects undertaken in this class continues to push the boundary of HCI innovation and research.
            </p>
            <p>
                Lastly, thank you Kevin Reuß for lending us your OpenBCI Cyton board for demoing purposes when our Ganglion Board unexpectedly stopped working! 
            </p>
          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
          <div class="row">
            <div class="col-lg-8 col-md-10 mx-auto">
                <div class="text-center">
                    Made with ♥ by Max Allen and Tim Chong
                </div>
            </div>
          </div>
        </div>
      </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/popper/popper.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
